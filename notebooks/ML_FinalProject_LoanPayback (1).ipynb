{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOB0Rwac2Rfh"
      },
      "source": [
        "Note: If code does not run properly, connect to T4 GPU runtime. **Models will take a while to run so *only* rerun code if neccessary.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Iu_OSUzLL5R"
      },
      "source": [
        "## 1. Install and Unpack Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLd3gZG9YEC2"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle\n",
        "#Assuming kaggle.json already in directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXJtLuNOYbGv",
        "outputId": "04f19f32-90d2-4e40-9937-3af9852c915f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 4, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/__init__.py\", line 6, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 434, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-DAYI-MZGBr",
        "outputId": "c157bf11-d4dc-4023-d8d5-df8dd0e37fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 4, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/__init__.py\", line 6, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 434, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n",
            "unzip:  cannot find or open /content/playground-series-s5e11.zip, /content/playground-series-s5e11.zip.zip or /content/playground-series-s5e11.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c playground-series-s5e11\n",
        "!unzip -q /content/playground-series-s5e11.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znQsWGXlfL7S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Define the base input path\n",
        "BASE_PATH = Path('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o08-jt6jLrsd"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "print('-'*50)\n",
        "print('Train CSV:')\n",
        "display(train_df.head())\n",
        "test_df = pd.read_csv('test.csv')\n",
        "print('-'*50)\n",
        "print('Test CSV:')\n",
        "display(test_df.head())\n",
        "sample_submission_df = pd.read_csv('sample_submission.csv')\n",
        "print('-'*50)\n",
        "print('Sample Submission CSV:')\n",
        "display(sample_submission_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Gn7azrw7jX1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2ld28snMGNI"
      },
      "source": [
        "## 2. EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll do some basic EDA on the data to gain a better understanding of what we're working with."
      ],
      "metadata": {
        "id": "jVVH-MGHNMtc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2maPAR-3Pbf5"
      },
      "outputs": [],
      "source": [
        "#summary statistics\n",
        "print(train_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KivKcPA_Msfq"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution of each column except 'id' using histograms\n",
        "train_df.drop('id', axis=1).hist(figsize=(15, 10))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju0Cr3mYNFkX"
      },
      "outputs": [],
      "source": [
        "#heatmap with correlation matrix\n",
        "corr = train_df.drop(['id', 'gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade'], axis=1).corr()\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVdMx28mNeaE"
      },
      "source": [
        "Most single variables have a minimal correlation with whether a loan is paid back, but `credit_score` has a moderate positive correlation, whereas `debt_to income_ratio` and `interest_rate` have quite notable negative correlations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds5lH-loN_BA"
      },
      "outputs": [],
      "source": [
        "# Box plots to visualize the relationship between key numerical features and loan_paid_back\n",
        "features_to_plot = ['debt_to_income_ratio', 'credit_score', 'interest_rate']\n",
        "\n",
        "for feature in features_to_plot:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.boxplot(x='loan_paid_back', y=feature, data=train_df)\n",
        "    plt.title(f'Distribution of {feature} by Loan Paid Back Status')\n",
        "    plt.xlabel('Loan Paid Back')\n",
        "    plt.ylabel(feature)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJgx4OhSOpCk"
      },
      "outputs": [],
      "source": [
        "categorical_features = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']\n",
        "\n",
        "for feature in categorical_features:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(x=feature, hue='loan_paid_back', data=train_df)\n",
        "    plt.title(f'Distribution of {feature} by Loan Paid Back Status')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek1AZo1GO-aU"
      },
      "source": [
        "In general, employment status and education level seem to have a noticeable impact toward the likelihood of whether loans will be paid back. Most loan purposes seem to have similar levels of loan payback likelihood, but debt consolidation has a noticeably higher likelihood of the loan *not* being paid back."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNTgHT4bPYtw"
      },
      "source": [
        "Now that we've done some simple exploratory statistics we can begin constructing the baseline models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X3JktIAPlqM"
      },
      "source": [
        "##3. Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "388x-828PpYF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate random predictions between 0 and 1 for the training data\n",
        "random_predictions = np.random.rand(len(train_df))\n",
        "\n",
        "# Calculate the AUC ROC score\n",
        "random_auc = roc_auc_score(train_df['loan_paid_back'], random_predictions)\n",
        "\n",
        "print(f\"AUC ROC for All-Random Baseline Model: {random_auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4f00fe9"
      },
      "outputs": [],
      "source": [
        "# Create a baseline model that always predicts the majority class (loan_paid_back = 1)\n",
        "majority_class_predictions = np.ones(len(train_df))\n",
        "\n",
        "# Calculate the AUC ROC score for the majority class baseline model\n",
        "majority_class_auc = roc_auc_score(train_df['loan_paid_back'], majority_class_predictions)\n",
        "\n",
        "print(f\"AUC ROC for Majority Class Baseline Model: {majority_class_auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-uWXTglP3B8"
      },
      "outputs": [],
      "source": [
        "# Create a baseline model that always predicts the minority class (loan_paid_back = 0)\n",
        "minority_class_predictions = np.zeros(len(train_df))\n",
        "\n",
        "# Calculate the AUC ROC score for the minority class baseline model\n",
        "minority_class_auc = roc_auc_score(train_df['loan_paid_back'], minority_class_predictions)\n",
        "\n",
        "print(f\"AUC ROC for Minority Class Baseline Model: {minority_class_auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqo8MxL9P95p"
      },
      "source": [
        "As would be expected, each baseline model boasts an AUC ROC of ~0.5, which indicates it will be right around half the time. For any of the actual models to be valuable, we will have to see AUC ROC greater than 0.5 to beat random guessing; ideally quite a bit higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n24KS7tLS13I"
      },
      "source": [
        "## 4. Logistic Regression with Cross Validation (Model A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO60udOOTIyU"
      },
      "source": [
        "First, we'll format the data for modelling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0lyTzeUQGyM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Separate target variable from features\n",
        "X = train_df.drop(['id', 'loan_paid_back'], axis=1)\n",
        "y = train_df['loan_paid_back']\n",
        "X_test = test_df.drop('id', axis=1)\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Create preprocessing pipelines for numerical and categorical features\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Create a column transformer to apply different transformations to different columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Apply preprocessing to the training and testing data\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Data preprocessing complete.\")\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_val:\", X_val.shape)\n",
        "print(\"Shape of X_test_processed:\", X_test_processed.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRE_l4GoO-Gw"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "logistic_model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the validation set\n",
        "y_val_pred_proba = logistic_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Calculate and print the AUC ROC score on the validation set\n",
        "logistic_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
        "\n",
        "print(f\"AUC ROC for Logistic Regression Model on Validation Set: {logistic_auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eZbGY8DmwSI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_val_pred_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Logistic Regression')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UfkxhBfToY5"
      },
      "source": [
        "The first simple logistic regression achieves an AUC ROC of ~0.9103, which is already quite promising. However, we should hope to see higher values still as we try more complex models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KegFcrjxT3_8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "logistic_model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Perform 5-fold cross-validation and calculate AUC ROC scores\n",
        "cv_scores = cross_val_score(logistic_model, X_processed, y, cv=5, scoring='roc_auc')\n",
        "\n",
        "print(f\"AUC ROC scores for each fold: {cv_scores}\")\n",
        "print(f\"Average AUC ROC score from 5-fold cross-validation: {cv_scores.mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpT_EtK2m5Nr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "logistic_model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "# Explicitly get the target variable from the training DataFrame to avoid interference\n",
        "y_train_cv = train_df['loan_paid_back']\n",
        "\n",
        "i = 0\n",
        "for train_index, test_index in kf.split(X_processed):\n",
        "    X_train_fold, X_test_fold = X_processed[train_index], X_processed[test_index]\n",
        "    y_train_fold, y_test_fold = y_train_cv.iloc[train_index], y_train_cv.iloc[test_index]\n",
        "\n",
        "\n",
        "    # Train the model on the current fold's training data\n",
        "    logistic_model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Predict probabilities on the current fold's test data\n",
        "    y_pred_proba_fold = logistic_model.predict_proba(X_test_fold)[:, 1]\n",
        "\n",
        "    # Calculate ROC curve and AUC for the current fold\n",
        "    fpr, tpr, thresholds = roc_curve(y_test_fold, y_pred_proba_fold)\n",
        "    aucs.append(auc(fpr, tpr))\n",
        "\n",
        "    # Interpolate the TPR for the mean FPR\n",
        "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
        "    tprs[-1][0] = 0.0\n",
        "\n",
        "    # Plot the ROC curve for the current fold\n",
        "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "             label=f'ROC fold {i+1} (AUC = {aucs[-1]:.2f})')\n",
        "\n",
        "    i += 1\n",
        "\n",
        "# Plot the random guessing line\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "         label='Random Guessing', alpha=.8)\n",
        "\n",
        "# Plot the mean ROC curve\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "         label=f'Mean ROC (AUC = {mean_auc:.2f} $\\\\pm$ {std_auc:.2f})',\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "# Plot the standard deviation band\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                 label='$\\\\pm$ 1 Std. Dev.')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curves for 5-fold Cross-Validation')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train a Logistic Regression model on the entire preprocessed training data\n",
        "logistic_model_full_data = LogisticRegression(solver='liblinear', random_state=42)\n",
        "logistic_model_full_data.fit(X_processed, y)\n",
        "\n",
        "# Get the coefficients from the trained model\n",
        "coefficients = logistic_model_full_data.coef_[0]\n",
        "\n",
        "# Get the feature names after preprocessing\n",
        "numerical_feature_names = numerical_features.tolist()\n",
        "categorical_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "all_feature_names = numerical_feature_names + categorical_feature_names\n",
        "\n",
        "# Create a DataFrame to display feature names and their coefficients\n",
        "feature_importance_df = pd.DataFrame({'Feature': all_feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "# Sort by absolute coefficient value to show most important features\n",
        "feature_importance_df['Abs_Coefficient'] = abs(feature_importance_df['Coefficient'])\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"Feature Importance (Coefficients) for Logistic Regression Model:\")\n",
        "display(feature_importance_df[['Feature', 'Coefficient']].head(20)) # Display top 20 features"
      ],
      "metadata": {
        "id": "YEOr7u6TN4AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Select the top N features to display\n",
        "top_n = 20\n",
        "top_features = feature_importance_df.head(top_n)\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Abs_Coefficient', y='Feature', data=top_features, palette='viridis')\n",
        "plt.title(f'Top {top_n} Feature Importance (Absolute Coefficients) for Logistic Regression')\n",
        "plt.xlabel('Absolute Coefficient Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4duy4qoOO8L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqvsFHB3U4ww"
      },
      "source": [
        "With 5-fold cross-validation, we see the model gain a slight increase in accuracy with an average AUC ROC of ~0.9106. While the evaluation metric is only slightly higher, the cross-validation model may also prevent overfitting, so the slightly higher performance strain will generally be worthwhile. In any case, both the cross-validation and simple logistic regression take less than a few minutes to run and produce moderately accurate results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also see that the importance of features for the logistic model reveals that employment status has the heaviest importance; retired and employed have high positive impacts on the likelihood of loan payback, whereas unemployed and student have significant negative impacts. As we were able to see in the correlation analysis, credit score and debt to income ratio also have high absolute coefficients. Combined with employment status, these are our primary predictors; however, many other variables have a smaller but vital impact."
      ],
      "metadata": {
        "id": "rnQEw5QPOi7r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JuYObh8Vmgc"
      },
      "source": [
        "## 5. Random Forest Classifier (Model B)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train a random forest to see if we can improve the accuracy of our predictions and reduce overfitting risks."
      ],
      "metadata": {
        "id": "PKbm4E4UP7Iv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0dt_D78V1oN"
      },
      "outputs": [],
      "source": [
        "from cuml.ensemble import RandomForestClassifier as cumlRandomForestClassifier\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "\n",
        "# Convert data to CuPy arrays for cuml\n",
        "# Ensure data is dense and float32\n",
        "X_train_gpu = cp.asarray(X_train.toarray().astype(np.float32) if hasattr(X_train, 'toarray') else X_train.astype(np.float32))\n",
        "y_train_gpu = cp.asarray(y_train.values.astype(np.float32) if hasattr(y_train, 'values') else y_train.astype(np.float32))\n",
        "X_val_gpu = cp.asarray(X_val.toarray().astype(np.float32) if hasattr(X_val, 'toarray') else X_val.astype(np.float32))\n",
        "y_val_gpu = cp.asarray(y_val.values.astype(np.float32) if hasattr(y_val, 'values') else y_val.astype(np.float32))\n",
        "\n",
        "# Initialize and train the cuml Random Forest Classifier model on GPU\n",
        "# Setting n_jobs=-1 might not be relevant for cuml as it uses GPU\n",
        "rf_model = cumlRandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_gpu, y_train_gpu)\n",
        "\n",
        "# Predict probabilities on the validation set (on GPU)\n",
        "y_val_pred_proba_rf_gpu = rf_model.predict_proba(X_val_gpu)[:, 1]\n",
        "\n",
        "# Convert predictions and true labels back to NumPy for AUC ROC calculation\n",
        "y_val_pred_proba_rf = cp.asnumpy(y_val_pred_proba_rf_gpu)\n",
        "y_val_cpu = cp.asnumpy(y_val_gpu)\n",
        "\n",
        "\n",
        "# Calculate and print the AUC ROC score on the validation set\n",
        "rf_auc = roc_auc_score(y_val_cpu, y_val_pred_proba_rf)\n",
        "\n",
        "print(f\"AUC ROC for cuml Random Forest Classifier Model on Validation Set (GPU): {rf_auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TnflLq0nXH2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the ROC curve for the Random Forest model\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_val, y_val_pred_proba_rf)\n",
        "\n",
        "# Calculate the AUC for the Random Forest model\n",
        "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
        "\n",
        "# Plot the ROC curve for the Random Forest model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_rf:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Random Forest Classifier')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crQqCGypdFzD"
      },
      "source": [
        "The simple Random Forest model's AUC ROC is higher than the basic logistic regression, and even 5-fold cross-validation at ~0.9110. This model is also likely more robust against overfitting than either model type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "904f7c24"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/env-check.py\n",
        "!bash rapidsai-csp-utils/colab/update_proxy.sh\n",
        "!conda install -c conda-forge cudatoolkit=11.8 --no-default-packages -y python=3.10 cudnn=8.7.0\n",
        "!conda install -c rapidsai -c conda-forge -c nvidia cuml=24.6 python=3.10 cudatoolkit=11.8 -y --allow-downgrades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "022327de"
      },
      "outputs": [],
      "source": [
        "import cuml\n",
        "print(\"cuml installed and imported.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed8b1d3e"
      },
      "outputs": [],
      "source": [
        "from cuml.ensemble import RandomForestClassifier as cumlRandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier # Import for potential CPU fallback\n",
        "\n",
        "# Convert preprocessed data to a dense NumPy array with a supported dtype before converting to CuPy\n",
        "# The preprocessor output is likely a sparse matrix, so convert to dense\n",
        "X_processed_dense = X_processed.toarray().astype(np.float32) if hasattr(X_processed, 'toarray') else X_processed.astype(np.float32)\n",
        "\n",
        "# Ensure the correct target variable is used before converting to CuPy\n",
        "y_train_data = train_df['loan_paid_back']\n",
        "\n",
        "# Convert data to CuPy arrays for cuml\n",
        "X_processed_gpu = cp.asarray(X_processed_dense)\n",
        "y_gpu = cp.asarray(y_train_data)\n",
        "\n",
        "\n",
        "# Initialize a cuml RandomForestClassifier\n",
        "# Use a smaller subset of the data for tuning due to potential memory constraints\n",
        "# and to speed up the tuning process if needed.\n",
        "# For demonstration, let's use a smaller parameter grid and potentially fewer data points if memory becomes an issue.\n",
        "cuml_rf_model = cumlRandomForestClassifier(random_state=42, n_estimators=100)\n",
        "\n",
        "# Define a smaller parameter grid for quicker demonstration\n",
        "param_grid_cuml = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_split': [2, 10]\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# Note: GridSearchCV from sklearn works with cuml estimators, but the data needs to be on the GPU.\n",
        "# For simplicity and compatibility, we will use the sklearn GridSearchCV with cuml model and cupy data.\n",
        "# Set n_jobs=1 to avoid multi-processing issues with GPU and GridSearchCV\n",
        "grid_search_cuml = GridSearchCV(estimator=cuml_rf_model, param_grid=param_grid_cuml, cv=3, scoring='roc_auc', n_jobs=1)\n",
        "\n",
        "# Fit GridSearchCV to the GPU data - explicitly convert CuPy to NumPy\n",
        "print(\"Starting GridSearchCV with cuml RandomForestClassifier...\")\n",
        "grid_search_cuml.fit(X_processed_gpu.get(), y_gpu.get())\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found: \", grid_search_cuml.best_params_)\n",
        "print(\"Best cross-validation AUC ROC score: \", grid_search_cuml.best_score_)\n",
        "\n",
        "# Optional: Convert best model back to CPU if needed later\n",
        "# best_cuml_model = grid_search_cuml.best_estimator_\n",
        "# best_cpu_model = RandomForestClassifier(n_estimators=best_cuml_model.n_estimators,\n",
        "#                                         max_depth=best_cpu_model.max_depth,\n",
        "#                                         min_samples_split=best_cpu_model.min_samples_split,\n",
        "#                                         random_state=42)\n",
        "# best_cpu_model.fit(X_processed, y) # Retrain on CPU data if needed for consistency or other operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRd5e49Dnhea"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy as cp\n",
        "\n",
        "# Get the best model from the GridSearchCV results\n",
        "best_cuml_model = grid_search_cuml.best_estimator_\n",
        "\n",
        "# Convert validation data to CuPy array\n",
        "X_val_gpu = cp.asarray(X_val.toarray().astype(np.float32) if hasattr(X_val, 'toarray') else X_val.astype(np.float32))\n",
        "\n",
        "# Predict probabilities on the validation set using the best cuml model\n",
        "# Ensure the model is in evaluation mode if applicable (though not standard for cuml RF)\n",
        "y_val_pred_proba_cuml_rf = best_cuml_model.predict_proba(X_val_gpu)[:, 1]\n",
        "\n",
        "# Convert predictions back to NumPy for plotting\n",
        "y_val_pred_proba_cuml_rf_cpu = cp.asnumpy(y_val_pred_proba_cuml_rf)\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr_cuml_rf, tpr_cuml_rf, thresholds_cuml_rf = roc_curve(y_val, y_val_pred_proba_cuml_rf_cpu)\n",
        "\n",
        "# Calculate the AUC\n",
        "roc_auc_cuml_rf = auc(fpr_cuml_rf, tpr_cuml_rf)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_cuml_rf, tpr_cuml_rf, color='darkorange', lw=2, label=f'ROC curve (AUC = {grid_search_cuml.best_score_:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Hyperparameter Tuned Random Forest (cuml)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygM9RwZmivGS"
      },
      "source": [
        "Using hyperparameter tuning on the random forest, we get the highest AUC ROC yet at ~0.9120. This is more computationally expensive than the prior models, but runs relatively quickly on GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxdSBUvojIKd"
      },
      "source": [
        "## 6. Neural Network Classifier (Model C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_1Kun15jy5Z"
      },
      "source": [
        "We will build a neural network using PyTorch to attempt to extract an even higher performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "zWj4sKhtbdYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ-M-U-AjWl0"
      },
      "outputs": [],
      "source": [
        "# Convert sparse arrays to dense arrays before converting to PyTorch tensors\n",
        "X_train_dense = X_train.todense() if hasattr(X_train, 'todense') else X_train\n",
        "X_val_dense = X_val.todense() if hasattr(X_val, 'todense') else X_val\n",
        "X_test_dense = X_test_processed.todense() if hasattr(X_test_processed, 'todense') else X_test_processed\n",
        "\n",
        "# Convert dense arrays and Pandas Series to PyTorch tensors and move to GPU\n",
        "X_train_tensor = torch.tensor(X_train_dense, dtype=torch.float32).to(device)\n",
        "X_val_tensor = torch.tensor(X_val_dense, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test_dense, dtype=torch.float32).to(device)\n",
        "\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).to(device)\n",
        "\n",
        "print(\"Data converted to dense arrays, then to PyTorch tensors and moved to GPU.\")\n",
        "print(\"Shape of X_train_tensor:\", X_train_tensor.shape)\n",
        "print(\"Shape of X_val_tensor:\", X_val_tensor.shape)\n",
        "print(\"Shape of X_test_tensor:\", X_test_tensor.shape)\n",
        "print(\"Shape of y_train_tensor:\", y_train_tensor.shape)\n",
        "print(\"Shape of y_val_tensor:\", y_val_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ac9f343"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the neural network class\n",
        "class LoanPredictor(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LoanPredictor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(64, 32)          # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(32, 1)           # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation after the first layer\n",
        "        x = F.relu(self.fc2(x))  # Apply ReLU activation after the second layer\n",
        "        x = torch.sigmoid(self.fc3(x)) # Apply sigmoid activation for binary classification\n",
        "        return x\n",
        "\n",
        "# Instantiate the model and move it to the device\n",
        "input_size = X_train_tensor.shape[1]\n",
        "model = LoanPredictor(input_size).to(device)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a302dd6"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss function (Binary Cross-Entropy)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Define the optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Loss function and optimizer defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5510d2b5"
      },
      "outputs": [],
      "source": [
        "# Set the model to training mode\n",
        "model.train()\n",
        "\n",
        "# Define the number of training epochs\n",
        "num_epochs = 100\n",
        "\n",
        "# Start the training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = criterion(outputs.squeeze(), y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss at regular intervals\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d2cc89d"
      },
      "outputs": [],
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculations\n",
        "with torch.no_grad():\n",
        "    # Get predictions on the validation set\n",
        "    y_val_pred_proba_nn = model(X_val_tensor)\n",
        "\n",
        "    # Move predictions and true labels to CPU and convert to NumPy arrays\n",
        "    y_val_pred_proba_nn_cpu = y_val_pred_proba_nn.squeeze().cpu().numpy()\n",
        "    y_val_cpu = y_val_tensor.cpu().numpy()\n",
        "\n",
        "    # Calculate the AUC ROC score\n",
        "    nn_auc = roc_auc_score(y_val_cpu, y_val_pred_proba_nn_cpu)\n",
        "\n",
        "    # Print the AUC ROC score\n",
        "    print(f\"AUC ROC for Neural Network Model on Validation Set: {nn_auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpZQ5yn9nt3p"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the ROC curve for the Neural Network model\n",
        "fpr_nn, tpr_nn, thresholds_nn = roc_curve(y_val_cpu, y_val_pred_proba_nn_cpu)\n",
        "\n",
        "# Calculate the AUC for the Neural Network model\n",
        "roc_auc_nn = auc(fpr_nn, tpr_nn)\n",
        "\n",
        "# Plot the ROC curve for the Neural Network model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_nn, tpr_nn, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_nn:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Neural Network')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSpHdPJakP5N"
      },
      "source": [
        "The basic neural network classifier reaches an AUC ROC of ~0.9005, a bit lower than most models we've seen so far. We will perform hyperparameter tuning to construct a model with a potentially higher metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2861c4b"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the neural network class with dropout\n",
        "class LoanPredictor(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.5):\n",
        "        super(LoanPredictor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(64, 32)          # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(32, 1)           # Output layer\n",
        "        self.dropout = nn.Dropout(dropout_rate) # Dropout layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation after the first layer\n",
        "        x = self.dropout(x)      # Apply dropout\n",
        "        x = F.relu(self.fc2(x))  # Apply ReLU activation after the second layer\n",
        "        x = self.dropout(x)      # Apply dropout\n",
        "        x = torch.sigmoid(self.fc3(x)) # Apply sigmoid activation for binary classification\n",
        "        return x\n",
        "\n",
        "# Instantiate the model and move it to the device\n",
        "input_size = X_train_tensor.shape[1]\n",
        "model = LoanPredictor(input_size, dropout_rate=0.5).to(device)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87ec77e1"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameter search space for the neural network\n",
        "param_grid_nn = {\n",
        "    'dropout_rate': [0.1, 0.3, 0.5],\n",
        "    'lr': [0.001, 0.005, 0.01]\n",
        "    # We could also tune layer sizes, but that would make the search space much larger\n",
        "}\n",
        "\n",
        "print(\"Hyperparameter search space defined:\")\n",
        "print(param_grid_nn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "765add41"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_and_evaluate_nn(dropout_rate, lr):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a neural network with given hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        dropout_rate (float): The dropout rate for the neural network.\n",
        "        lr (float): The learning rate for the optimizer.\n",
        "\n",
        "    Returns:\n",
        "        float: The AUC ROC score on the validation set.\n",
        "    \"\"\"\n",
        "    # Instantiate the model with the given dropout rate\n",
        "    model = LoanPredictor(input_size, dropout_rate=dropout_rate).to(device)\n",
        "\n",
        "    # Define the loss function and optimizer with the given learning rate\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Define the number of training epochs\n",
        "    num_epochs = 100\n",
        "\n",
        "    # Start the training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Forward pass\n",
        "        outputs = model(X_train_tensor)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(outputs.squeeze(), y_train_tensor)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient calculations\n",
        "    with torch.no_grad():\n",
        "        # Get predictions on the validation set\n",
        "        y_val_pred_proba_nn = model(X_val_tensor)\n",
        "\n",
        "        # Move predictions and true labels to CPU and convert to NumPy arrays\n",
        "        y_val_pred_proba_nn_cpu = y_val_pred_proba_nn.squeeze().cpu().numpy()\n",
        "        y_val_cpu = y_val_tensor.cpu().numpy()\n",
        "\n",
        "        # Calculate the AUC ROC score\n",
        "        nn_auc = roc_auc_score(y_val_cpu, y_val_pred_proba_nn_cpu)\n",
        "\n",
        "    return nn_auc\n",
        "\n",
        "print(\"Training and evaluation function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b2b9cc0"
      },
      "outputs": [],
      "source": [
        "# Initialize variables to store the best AUC ROC score and hyperparameters\n",
        "best_auc_nn = 0\n",
        "best_params_nn = {}\n",
        "\n",
        "# Iterate through each combination of hyperparameters\n",
        "for dropout_rate in param_grid_nn['dropout_rate']:\n",
        "    for lr in param_grid_nn['lr']:\n",
        "        print(f\"Training with dropout_rate={dropout_rate}, lr={lr}\")\n",
        "        # Train and evaluate the model\n",
        "        current_auc = train_and_evaluate_nn(dropout_rate, lr)\n",
        "\n",
        "        # Print the AUC ROC score for the current combination\n",
        "        print(f\"AUC ROC for dropout_rate={dropout_rate}, lr={lr}: {current_auc:.4f}\")\n",
        "\n",
        "        # Compare and update best parameters if current score is better\n",
        "        if current_auc > best_auc_nn:\n",
        "            best_auc_nn = current_auc\n",
        "            best_params_nn = {'dropout_rate': dropout_rate, 'lr': lr}\n",
        "            print(\"New best AUC ROC found!\")\n",
        "\n",
        "# Print the best AUC ROC score and corresponding hyperparameters\n",
        "print(\"\\n--- Hyperparameter Tuning Complete ---\")\n",
        "print(f\"Best AUC ROC: {best_auc_nn:.4f}\")\n",
        "print(f\"Best Hyperparameters: {best_params_nn}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mpAminxkk3L"
      },
      "outputs": [],
      "source": [
        "# Instantiate the final model with the best hyperparameters\n",
        "final_model = LoanPredictor(input_size, dropout_rate=best_params_nn['dropout_rate']).to(device)\n",
        "\n",
        "# Define the loss function and optimizer with the best learning rate\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(final_model.parameters(), lr=best_params_nn['lr'])\n",
        "\n",
        "# Set the model to training mode\n",
        "final_model.train()\n",
        "\n",
        "# Define the number of training epochs\n",
        "num_epochs = 100 # Can increase if needed\n",
        "\n",
        "# Convert CuPy arrays to PyTorch tensors and move to the correct device\n",
        "X_processed_tensor = torch.tensor(X_processed_gpu.get(), dtype=torch.float32).to(device)\n",
        "y_tensor = torch.tensor(y_gpu.get(), dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "# Start the training loop on the entire training data\n",
        "print(f\"Training the final model with best hyperparameters: {best_params_nn}\")\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = final_model(X_processed_tensor)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = criterion(outputs.squeeze(), y_tensor)\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss at regular intervals\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Final model training complete.\")\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "final_model.eval()\n",
        "print(\"Final model set to evaluation mode.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce27d3a8"
      },
      "outputs": [],
      "source": [
        "# Set the final model to evaluation mode (already done in the previous step, but good practice to ensure)\n",
        "final_model.eval()\n",
        "\n",
        "# Disable gradient calculations\n",
        "with torch.no_grad():\n",
        "    # Get predictions on the validation set using the final model\n",
        "    y_val_pred_proba_final_nn = final_model(X_val_tensor)\n",
        "\n",
        "    # Move predictions and true labels to CPU and convert to NumPy arrays\n",
        "    y_val_pred_proba_final_nn_cpu = y_val_pred_proba_final_nn.squeeze().cpu().numpy()\n",
        "    y_val_cpu = y_val_tensor.cpu().numpy()\n",
        "\n",
        "    # Calculate the AUC ROC score for the final model on the validation set\n",
        "    final_nn_auc = roc_auc_score(y_val_cpu, y_val_pred_proba_final_nn_cpu)\n",
        "\n",
        "# Print the AUC ROC score for the final model\n",
        "print(f\"AUC ROC for Final Neural Network Model on Validation Set: {final_nn_auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwqj_RJZoRo_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Set the final model to evaluation mode (already done in the previous step, but good practice to ensure)\n",
        "final_model.eval()\n",
        "\n",
        "# Disable gradient calculations\n",
        "with torch.no_grad():\n",
        "    # Get predictions on the validation set using the final model\n",
        "    y_val_pred_proba_final_nn = final_model(X_val_tensor)\n",
        "\n",
        "    # Move predictions and true labels to CPU and convert to NumPy arrays\n",
        "    y_val_pred_proba_final_nn_cpu = y_val_pred_proba_final_nn.squeeze().cpu().numpy()\n",
        "    y_val_cpu = y_val_tensor.cpu().numpy() # Ensure y_val_cpu is the same as used for calculating nn_auc\n",
        "\n",
        "# Calculate the ROC curve for the final Neural Network model\n",
        "fpr_final_nn, tpr_final_nn, thresholds_final_nn = roc_curve(y_val_cpu, y_val_pred_proba_final_nn_cpu)\n",
        "\n",
        "# Calculate the AUC for the final Neural Network model\n",
        "roc_auc_final_nn = auc(fpr_final_nn, tpr_final_nn)\n",
        "\n",
        "# Plot the ROC curve for the final Neural Network model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_final_nn, tpr_final_nn, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_final_nn:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Hyperparameter Tuned Neural Network')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvjGOLwOlKN9"
      },
      "source": [
        "This neural network model garners an AUC ROC of ~0.9116, which is just barely lower than the random forest after hyperparameter tuning. Overall, it is slightly more resource-intensive than the random forest model. The differences between the two models are minimal, although the performance and accuracy of the neural network is a bit poorer, so the random forest is still the best model so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6F9Jc8ZqVft"
      },
      "source": [
        "## 7. XGBoost (Model D)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use a more advanced XGBoost model and start with some basic feature engineering to improve its performance."
      ],
      "metadata": {
        "id": "gfz1AOXNRjlj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ56odYjqgJl"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Convert X_processed back to a DataFrame to easily create interaction terms\n",
        "# Note: This might require handling the column names from the OneHotEncoder\n",
        "# A simpler approach for demonstration is to work with the original numerical features\n",
        "# and then combine with the processed categorical features.\n",
        "\n",
        "# Let's focus on creating interaction terms from the original numerical features first\n",
        "X_numerical = train_df[numerical_features]\n",
        "X_test_numerical = test_df[numerical_features]\n",
        "\n",
        "# Create polynomial features (including interaction terms) for numerical features\n",
        "# Degree 2 will create terms like x^2 and x*y\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "X_numerical_poly = poly.fit_transform(X_numerical)\n",
        "X_test_numerical_poly = poly.transform(X_test_numerical)\n",
        "\n",
        "# The column names for polynomial features are not automatically generated in a user-friendly way.\n",
        "# For simplicity in combining, we can convert the processed data back to DataFrames.\n",
        "# A more robust approach would involve creating a custom transformer.\n",
        "\n",
        "# Let's re-apply the preprocessing pipeline and then add new features\n",
        "# We need to get the feature names after one-hot encoding\n",
        "\n",
        "# Get feature names after preprocessing\n",
        "# This requires fitting the preprocessor again or accessing its fitted state\n",
        "# A simpler way is to get the feature names from the fitted OneHotEncoder\n",
        "# and combine with numerical feature names\n",
        "\n",
        "numerical_feature_names = numerical_features.tolist()\n",
        "categorical_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "all_feature_names = numerical_feature_names + categorical_feature_names\n",
        "\n",
        "# Convert processed data to dense NumPy arrays before converting to DataFrame\n",
        "X_processed_dense = X_processed.toarray() if hasattr(X_processed, 'toarray') else X_processed\n",
        "X_test_processed_dense = X_test_processed.toarray() if hasattr(X_test_processed, 'toarray') else X_test_processed\n",
        "\n",
        "# Convert dense arrays back to DataFrame to add new features\n",
        "X_processed_df = pd.DataFrame(X_processed_dense, columns=all_feature_names)\n",
        "X_test_processed_df = pd.DataFrame(X_test_processed_dense, columns=all_feature_names)\n",
        "\n",
        "\n",
        "# Add interaction terms between selected numerical features\n",
        "X_processed_df['income_credit_interaction'] = X_processed_df['annual_income'] * X_processed_df['credit_score']\n",
        "X_processed_df['debt_credit_interaction'] = X_processed_df['debt_to_income_ratio'] * X_processed_df['credit_score']\n",
        "X_processed_df['income_debt_interaction'] = X_processed_df['annual_income'] * X_processed_df['debt_to_income_ratio']\n",
        "X_processed_df['credit_interest_interaction'] = X_processed_df['credit_score'] * X_processed_df['interest_rate']\n",
        "\n",
        "\n",
        "X_test_processed_df['income_credit_interaction'] = X_test_processed_df['annual_income'] * X_test_processed_df['credit_score']\n",
        "X_test_processed_df['debt_credit_interaction'] = X_test_processed_df['debt_to_income_ratio'] * X_test_processed_df['credit_score']\n",
        "X_test_processed_df['income_debt_interaction'] = X_test_processed_df['annual_income'] * X_test_processed_df['debt_to_income_ratio']\n",
        "X_test_processed_df['credit_interest_interaction'] = X_test_processed_df['credit_score'] * X_test_processed_df['interest_rate']\n",
        "\n",
        "\n",
        "print(\"Feature engineering complete. Added interaction terms.\")\n",
        "print(\"Shape of X_processed_df:\", X_processed_df.shape)\n",
        "print(\"Shape of X_test_processed_df:\", X_test_processed_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88f1e8bd"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Apply the same feature engineering steps to the train, validation, and test sets\n",
        "# This is needed because the splits were created before feature engineering\n",
        "\n",
        "# Convert sparse arrays to dense NumPy arrays before creating DataFrames\n",
        "X_train_dense = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
        "X_val_dense = X_val.toarray() if hasattr(X_val, 'toarray') else X_val\n",
        "X_test_processed_dense = X_test_processed.toarray() if hasattr(X_test_processed, 'toarray') else X_test_processed\n",
        "\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train_dense, columns=all_feature_names)\n",
        "X_val_df = pd.DataFrame(X_val_dense, columns=all_feature_names)\n",
        "X_test_processed_df_recreated = pd.DataFrame(X_test_processed_dense, columns=all_feature_names)\n",
        "\n",
        "\n",
        "X_train_df['income_credit_interaction'] = X_train_df['annual_income'] * X_train_df['credit_score']\n",
        "X_train_df['debt_credit_interaction'] = X_train_df['debt_to_income_ratio'] * X_train_df['credit_score']\n",
        "X_train_df['income_debt_interaction'] = X_train_df['annual_income'] * X_train_df['debt_to_income_ratio']\n",
        "X_train_df['credit_interest_interaction'] = X_train_df['credit_score'] * X_train_df['interest_rate']\n",
        "\n",
        "X_val_df['income_credit_interaction'] = X_val_df['annual_income'] * X_val_df['credit_score']\n",
        "X_val_df['debt_credit_interaction'] = X_val_df['debt_to_income_ratio'] * X_val_df['credit_score']\n",
        "X_val_df['income_debt_interaction'] = X_val_df['annual_income'] * X_val_df['debt_to_income_ratio']\n",
        "X_val_df['credit_interest_interaction'] = X_val_df['credit_score'] * X_val_df['interest_rate']\n",
        "\n",
        "X_test_processed_df_recreated['income_credit_interaction'] = X_test_processed_df_recreated['annual_income'] * X_test_processed_df_recreated['credit_score']\n",
        "X_test_processed_df_recreated['debt_credit_interaction'] = X_test_processed_df_recreated['debt_to_income_ratio'] * X_test_processed_df_recreated['credit_score']\n",
        "X_test_processed_df_recreated['income_debt_interaction'] = X_test_processed_df_recreated['annual_income'] * X_test_processed_df_recreated['debt_to_income_ratio']\n",
        "X_test_processed_df_recreated['credit_interest_interaction'] = X_test_processed_df_recreated['credit_score'] * X_test_processed_df_recreated['interest_rate']\n",
        "\n",
        "\n",
        "# Initialize and train the XGBoost model\n",
        "# Using default hyperparameters for a baseline XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "print(\"Starting XGBoost model training...\")\n",
        "xgb_model.fit(X_train_df, y_train)\n",
        "print(\"XGBoost model training complete.\")\n",
        "\n",
        "# Predict probabilities on the validation set\n",
        "y_val_pred_proba_xgb = xgb_model.predict_proba(X_val_df)[:, 1]\n",
        "\n",
        "# Calculate and print the AUC ROC score on the validation set\n",
        "xgb_auc = roc_auc_score(y_val, y_val_pred_proba_xgb)\n",
        "\n",
        "print(f\"AUC ROC for XGBoost Model on Validation Set: {xgb_auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the ROC curve for the simple XGBoost model\n",
        "fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_val, y_val_pred_proba_xgb)\n",
        "\n",
        "# Calculate the AUC for the simple XGBoost model\n",
        "roc_auc_xgb = auc(fpr_xgb, tpr_xgb)\n",
        "\n",
        "# Plot the ROC curve for the simple XGBoost model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_xgb, tpr_xgb, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_xgb:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Simple XGBoost Model')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EXaPtTZIUP7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nmu8En6raHW"
      },
      "source": [
        "The basic XGBoost model continues to improve upon our efforts with an AUC ROC of ~0.9194, the best score yet. We will attempt to improve this further with hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r09nv2yDrpPp"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Initialize XGBoost Classifier with GPU support\n",
        "# tree_method='hist' is generally faster and compatible with GPU\n",
        "# device='cuda' explicitly tells XGBoost to use the GPU\n",
        "xgb_model_gpu = xgb.XGBClassifier(objective='binary:logistic', random_state=42,\n",
        "                                  use_label_encoder=False, eval_metric='logloss',\n",
        "                                  tree_method='hist', device='cuda')\n",
        "\n",
        "# Define a smaller hyperparameter grid for tuning with GPU\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'max_depth': [3, 5],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# Set n_jobs=1 as the GPU handles parallelism within XGBoost\n",
        "grid_search_xgb = GridSearchCV(estimator=xgb_model_gpu, param_grid=param_grid_xgb,\n",
        "                             cv=3, scoring='roc_auc', n_jobs=1, verbose=2)\n",
        "\n",
        "print(\"Starting GridSearchCV for XGBoost with GPU acceleration...\")\n",
        "\n",
        "# Fit GridSearchCV to the training data with engineered features\n",
        "grid_search_xgb.fit(X_train_df, y_train)\n",
        "\n",
        "print(\"\\n--- Hyperparameter Tuning Complete ---\")\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found: \", grid_search_xgb.best_params_)\n",
        "print(\"Best cross-validation AUC ROC score: \", grid_search_xgb.best_score_)\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "best_xgb_model = grid_search_xgb.best_estimator_\n",
        "y_val_pred_proba_best_xgb = best_xgb_model.predict_proba(X_val_df)[:, 1]\n",
        "best_xgb_auc = roc_auc_score(y_val, y_val_pred_proba_best_xgb)\n",
        "\n",
        "print(f\"AUC ROC for Best XGBoost Model on Validation Set: {best_xgb_auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYyZyAKct1N1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the best model from the GridSearchCV results\n",
        "best_xgb_model = grid_search_xgb.best_estimator_\n",
        "\n",
        "# Predict probabilities on the validation set using the best XGBoost model\n",
        "y_val_pred_proba_best_xgb = best_xgb_model.predict_proba(X_val_df)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr_best_xgb, tpr_best_xgb, thresholds_best_xgb = roc_curve(y_val, y_val_pred_proba_best_xgb)\n",
        "\n",
        "# Calculate the AUC\n",
        "roc_auc_best_xgb = auc(fpr_best_xgb, tpr_best_xgb)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_best_xgb, tpr_best_xgb, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_best_xgb:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Hyperparameter Tuned XGBoost Model')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFRC5u1Ws-Tp"
      },
      "source": [
        "The XGBoost hyperparameter method actually has a bit lower AUC ROC value than the simple method (~0.9187), but may be less overfitted and actually perform better in classifying unforseen data. However, this is one of the most computationally expensive methods yet, so its accuracy comes at a price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUlovJi3tyf3"
      },
      "source": [
        "## 8. LightGBM (Model E)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa2ee51c"
      },
      "outputs": [],
      "source": [
        "!pip install lightgbm==3.3.2\n",
        "import lightgbm as lgb\n",
        "print(lgb.__version__)\n",
        "\n",
        "try:\n",
        "    # Attempt to initialize a model with device='gpu' to check for GPU support\n",
        "    lgb.LGBMClassifier(device='gpu')\n",
        "    print(\"LightGBM GPU support is available.\")\n",
        "except Exception as e:\n",
        "    print(f\"LightGBM GPU support is not available. Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7792b302"
      },
      "outputs": [],
      "source": [
        "# Convert DataFrames to NumPy arrays\n",
        "X_train_np = X_train_df.values.astype(np.float32)\n",
        "X_val_np = X_val_df.values.astype(np.float32)\n",
        "X_test_processed_np = X_test_processed_df.values.astype(np.float32)\n",
        "\n",
        "# Convert target variable Series to NumPy arrays\n",
        "y_train_np = y_train.values.astype(np.float32)\n",
        "y_val_np = y_val.values.astype(np.float32)\n",
        "\n",
        "# Confirm the shapes\n",
        "print(\"Shape of X_train_np:\", X_train_np.shape)\n",
        "print(\"Shape of X_val_np:\", X_val_np.shape)\n",
        "print(\"Shape of X_test_processed_np:\", X_test_processed_np.shape)\n",
        "print(\"Shape of y_train_np:\", y_train_np.shape)\n",
        "print(\"Shape of y_val_np:\", y_val_np.shape)\n",
        "\n",
        "# Confirm the data types\n",
        "print(\"\\nDtype of X_train_np:\", X_train_np.dtype)\n",
        "print(\"Dtype of X_val_np:\", X_val_np.dtype)\n",
        "print(\"Dtype of X_test_processed_np:\", X_test_processed_np.dtype)\n",
        "print(\"Dtype of y_train_np:\", y_train_np.dtype)\n",
        "print(\"Dtype of y_val_np:\", y_val_np.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f910253a"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Initialize an LGBMClassifier object without explicit GPU acceleration\n",
        "lgbm_model = LGBMClassifier(objective='binary', metric='auc', random_state=42)\n",
        "\n",
        "# Train the LightGBM model on the training data\n",
        "print(\"Starting LightGBM model training without GPU...\")\n",
        "lgbm_model.fit(X_train_np, y_train_np)\n",
        "print(\"LightGBM model training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9HRUjLlvBKi"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Predict probabilities on the validation set\n",
        "y_val_pred_proba_lgbm = lgbm_model.predict_proba(X_val_np)[:, 1]\n",
        "\n",
        "# Calculate and print the AUC ROC score on the validation set\n",
        "lgbm_auc = roc_auc_score(y_val_np, y_val_pred_proba_lgbm)\n",
        "\n",
        "print(f\"AUC ROC for LightGBM Model on Validation Set: {lgbm_auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpGKZyAwvEj9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr_lgbm, tpr_lgbm, thresholds_lgbm = roc_curve(y_val_np, y_val_pred_proba_lgbm)\n",
        "\n",
        "# Calculate the AUC\n",
        "roc_auc_lgbm = auc(fpr_lgbm, tpr_lgbm)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_lgbm, tpr_lgbm, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_lgbm:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for LightGBM Model')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsVZJhz2vLSr"
      },
      "source": [
        "The LightGBM model performs with an AUC ROC of ~0.9183, marginally poorer than the XGBoost after hyperparameter tuning. However, the model can be executed relatively quickly compared to other alternatives."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Gaussian Naive Bayes Classification (Model F)"
      ],
      "metadata": {
        "id": "RmFWjKX14nuP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88117c9c"
      },
      "source": [
        "import cupy as cp\n",
        "\n",
        "# Convert sparse X_train and X_val to dense NumPy arrays (if they are sparse) and ensure float32 dtype\n",
        "X_train_dense_np = X_train.toarray().astype(np.float32) if hasattr(X_train, 'toarray') else X_train.astype(np.float32)\n",
        "X_val_dense_np = X_val.toarray().astype(np.float32) if hasattr(X_val, 'toarray') else X_val.astype(np.float32)\n",
        "\n",
        "# Convert y_train and y_val to NumPy arrays and ensure float32 dtype\n",
        "y_train_np = y_train.values.astype(np.float32)\n",
        "y_val_np = y_val.values.astype(np.float32)\n",
        "\n",
        "# Convert NumPy arrays to CuPy arrays\n",
        "X_train_nb_gpu = cp.asarray(X_train_dense_np)\n",
        "y_train_nb_gpu = cp.asarray(y_train_np)\n",
        "X_val_nb_gpu = cp.asarray(X_val_dense_np)\n",
        "y_val_nb_gpu = cp.asarray(y_val_np)\n",
        "\n",
        "print(\"Data converted to dense CuPy arrays with float32 dtype.\")\n",
        "print(f\"Shape of X_train_nb_gpu: {X_train_nb_gpu.shape}, dtype: {X_train_nb_gpu.dtype}\")\n",
        "print(f\"Shape of y_train_nb_gpu: {y_train_nb_gpu.shape}, dtype: {y_train_nb_gpu.dtype}\")\n",
        "print(f\"Shape of X_val_nb_gpu: {X_val_nb_gpu.shape}, dtype: {X_val_nb_gpu.dtype}\")\n",
        "print(f\"Shape of y_val_nb_gpu: {y_val_nb_gpu.shape}, dtype: {y_val_nb_gpu.dtype}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d5daa61"
      },
      "source": [
        "from cuml.naive_bayes import GaussianNB\n",
        "\n",
        "# Instantiate the Gaussian Naive Bayes model\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "print(\"Starting cuML Gaussian Naive Bayes model training...\")\n",
        "# Train the model using the GPU-acceleraccelerated training data\n",
        "nb_model.fit(X_train_nb_gpu, y_train_nb_gpu)\n",
        "print(\"cuML Gaussian Naive Bayes model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fcfd8b5"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import cupy as cp\n",
        "\n",
        "# Predict probabilities on the validation set using the trained model\n",
        "y_val_pred_proba_nb_gpu = nb_model.predict_proba(X_val_nb_gpu)[:, 1]\n",
        "\n",
        "# Convert CuPy predictions and true labels back to NumPy for AUC ROC calculation\n",
        "y_val_pred_proba_nb_cpu = cp.asnumpy(y_val_pred_proba_nb_gpu)\n",
        "y_val_cpu = cp.asnumpy(y_val_nb_gpu)\n",
        "\n",
        "# Calculate and print the AUC ROC score on the validation set\n",
        "nb_auc = roc_auc_score(y_val_cpu, y_val_pred_proba_nb_cpu)\n",
        "\n",
        "print(f\"AUC ROC for cuML Gaussian Naive Bayes Model on Validation Set: {nb_auc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "439b6efa"
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the ROC curve for the Naive Bayes model\n",
        "fpr_nb, tpr_nb, thresholds_nb = roc_curve(y_val_cpu, y_val_pred_proba_nb_cpu)\n",
        "\n",
        "# Calculate the AUC for the Naive Bayes model\n",
        "roc_auc_nb = auc(fpr_nb, tpr_nb)\n",
        "\n",
        "# Plot the ROC curve for the Naive Bayes model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_nb, tpr_nb, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_nb:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for cuML Gaussian Naive Bayes Model')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Naive Bayes model actually performs with the poorest results yet, so we will attempt to improve its ROC AUC score with hyperparameter tuning next."
      ],
      "metadata": {
        "id": "8PR-yKT36_Ix"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c568bdc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the hyperparameter search space for var_smoothing\n",
        "param_grid_nb = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]}\n",
        "\n",
        "print(\"Hyperparameter search space for var_smoothing defined:\")\n",
        "print(param_grid_nb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "656cbb67"
      },
      "source": [
        "from cuml.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import cupy as cp\n",
        "\n",
        "# Initialize variables to store the best AUC ROC score and hyperparameters\n",
        "best_auc_nb = 0\n",
        "best_params_nb = {}\n",
        "\n",
        "# Iterate through each var_smoothing value\n",
        "print(\"Starting hyperparameter tuning for Gaussian Naive Bayes...\")\n",
        "for var_smoothing_val in param_grid_nb['var_smoothing']:\n",
        "    print(f\"\\nTraining with var_smoothing={var_smoothing_val}\")\n",
        "    # Instantiate the Gaussian Naive Bayes model with the current var_smoothing\n",
        "    current_nb_model = GaussianNB(var_smoothing=var_smoothing_val)\n",
        "\n",
        "    # Train the model using the GPU-accelerated training data\n",
        "    current_nb_model.fit(X_train_nb_gpu, y_train_nb_gpu)\n",
        "\n",
        "    # Predict probabilities on the validation set (on GPU)\n",
        "    y_val_pred_proba_current_nb_gpu = current_nb_model.predict_proba(X_val_nb_gpu)[:, 1]\n",
        "\n",
        "    # Convert predictions and true labels back to NumPy for AUC ROC calculation\n",
        "    y_val_pred_proba_current_nb_cpu = cp.asnumpy(y_val_pred_proba_current_nb_gpu)\n",
        "    # y_val_cpu is already available from previous cells\n",
        "\n",
        "    # Calculate the AUC ROC score for the current combination\n",
        "    current_auc_nb = roc_auc_score(y_val_cpu, y_val_pred_proba_current_nb_cpu)\n",
        "\n",
        "    print(f\"AUC ROC for var_smoothing={var_smoothing_val}: {current_auc_nb:.4f}\")\n",
        "\n",
        "    # Compare and update best parameters if current score is better\n",
        "    if current_auc_nb > best_auc_nb:\n",
        "        best_auc_nb = current_auc_nb\n",
        "        best_params_nb = {'var_smoothing': var_smoothing_val}\n",
        "        print(\"New best AUC ROC found!\")\n",
        "\n",
        "print(\"\\n--- Hyperparameter Tuning Complete ---\")\n",
        "print(f\"Best AUC ROC: {best_auc_nb:.4f}\")\n",
        "print(f\"Best Hyperparameters: {best_params_nb}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb515193"
      },
      "source": [
        "from cuml.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import cupy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Instantiate a cuml.naive_bayes.GaussianNB model using the var_smoothing value obtained from the hyperparameter tuning\n",
        "final_nb_model = GaussianNB(var_smoothing=best_params_nb['var_smoothing'])\n",
        "\n",
        "# 2. Train this model on the GPU-accelerated training data\n",
        "print(f\"\\nTraining final cuML Gaussian Naive Bayes model with var_smoothing={best_params_nb['var_smoothing']}...\")\n",
        "final_nb_model.fit(X_train_nb_gpu, y_train_nb_gpu)\n",
        "print(\"Final cuML Gaussian Naive Bayes model training complete.\")\n",
        "\n",
        "# 3. Predict probabilities on the GPU-accelerated validation set\n",
        "y_val_pred_proba_final_nb_gpu = final_nb_model.predict_proba(X_val_nb_gpu)[:, 1]\n",
        "\n",
        "# 4. Convert the predicted probabilities and true validation labels to NumPy arrays\n",
        "y_val_pred_proba_final_nb_cpu = cp.asnumpy(y_val_pred_proba_final_nb_gpu)\n",
        "y_val_cpu = cp.asnumpy(y_val_nb_gpu) # Ensure y_val_cpu is available and correctly represents the true labels\n",
        "\n",
        "# 5. Calculate the AUC ROC score for the tuned model\n",
        "final_nb_auc = roc_auc_score(y_val_cpu, y_val_pred_proba_final_nb_cpu)\n",
        "\n",
        "# 6. Print the AUC ROC score\n",
        "print(f\"AUC ROC for Hyperparameter Tuned cuML Gaussian Naive Bayes Model on Validation Set: {final_nb_auc}\")\n",
        "\n",
        "# 7. Calculate the False Positive Rate (FPR), True Positive Rate (TPR), and thresholds\n",
        "fpr_final_nb, tpr_final_nb, thresholds_final_nb = roc_curve(y_val_cpu, y_val_pred_proba_final_nb_cpu)\n",
        "\n",
        "# Calculate the AUC for the final Naive Bayes model\n",
        "roc_auc_final_nb = auc(fpr_final_nb, tpr_final_nb)\n",
        "\n",
        "# 8. Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_final_nb, tpr_final_nb, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_final_nb:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Hyperparameter Tuned cuML Gaussian Naive Bayes Model')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tuning, the bayes model still does not perform particularly well, even worse than the simple logistic regression at ~0.8981. However, it may be useful later when we compile hybrid models due to minimizing certain other models' weaknesses, particularly XGBoost which we will focus on for hybrids:"
      ],
      "metadata": {
        "id": "VcCbT_js7Htq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Hybrid Models"
      ],
      "metadata": {
        "id": "pWm_ymy-Sxv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import cupy as cp\n",
        "\n",
        "# Get the best trained models from previous steps\n",
        "# Assuming best_xgb_model and best_cuml_model are available from previous cells\n",
        "# best_xgb_model = grid_search_xgb.best_estimator_\n",
        "# best_cuml_model = grid_search_cuml.best_estimator_\n",
        "\n",
        "# Make predictions on the validation set using the best XGBoost model\n",
        "# Ensure X_val_df is available from the XGBoost feature engineering step\n",
        "y_val_pred_proba_xgb_tuned = best_xgb_model.predict_proba(X_val_df)[:, 1]\n",
        "\n",
        "# Make predictions on the validation set using the best cuml Random Forest model\n",
        "# Ensure X_val_gpu is available from the cuml RF setup (convert X_val to CuPy if not)\n",
        "# Assuming X_val_gpu was created in cell Z0dt_D78V1oN or ZRd5e49Dnhea\n",
        "if 'X_val_gpu' not in locals():\n",
        "    X_val_gpu = cp.asarray(X_val.toarray().astype(np.float32) if hasattr(X_val, 'toarray') else X_val.astype(np.float32))\n",
        "\n",
        "y_val_pred_proba_rf_tuned_gpu = best_cuml_model.predict_proba(X_val_gpu)[:, 1]\n",
        "y_val_pred_proba_rf_tuned = cp.asnumpy(y_val_pred_proba_rf_tuned_gpu)\n",
        "\n",
        "\n",
        "# Combine the predictions (simple averaging)\n",
        "hybrid_predictions_proba = (y_val_pred_proba_xgb_tuned + y_val_pred_proba_rf_tuned) / 2\n",
        "\n",
        "# Calculate and print the AUC ROC score for the hybrid model\n",
        "hybrid_auc = roc_auc_score(y_val, hybrid_predictions_proba)\n",
        "\n",
        "print(f\"AUC ROC for Hybrid XGBoost + Random Forest Model on Validation Set: {hybrid_auc}\")"
      ],
      "metadata": {
        "id": "oftr9H8-S4tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import cupy as cp\n",
        "\n",
        "# Make predictions on the training set using the best XGBoost model\n",
        "# Ensure X_processed_df is available from the XGBoost feature engineering step\n",
        "y_train_pred_proba_xgb_tuned = best_xgb_model.predict_proba(X_processed_df)[:, 1]\n",
        "\n",
        "# Make predictions on the training set using the best cuml Random Forest model\n",
        "# Ensure X_processed_gpu is available from the cuml RF setup (convert X_processed_df to CuPy if not)\n",
        "X_processed_gpu = cp.asarray(X_processed_df.values.astype(np.float32))\n",
        "\n",
        "y_train_pred_proba_rf_tuned_gpu = best_cuml_model.predict_proba(X_processed_gpu)[:, 1]\n",
        "y_train_pred_proba_rf_tuned = cp.asnumpy(y_train_pred_proba_rf_tuned_gpu)\n",
        "\n",
        "# Combine the predictions (simple averaging)\n",
        "hybrid_train_predictions_proba = (y_train_pred_proba_xgb_tuned + y_train_pred_proba_rf_tuned) / 2\n",
        "\n",
        "# Calculate and print the AUC ROC score for the hybrid model on the training set\n",
        "hybrid_train_auc = roc_auc_score(y, hybrid_train_predictions_proba)\n",
        "\n",
        "print(f\"AUC ROC for Hybrid XGBoost + Random Forest Model on Training Set: {hybrid_train_auc}\")"
      ],
      "metadata": {
        "id": "LGqcdsiLW_5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DT-XGBoost hybrid model has a high AUC ROC on the validation set, but much lower on the training set. While the validation set might give us a better idea of the actual performance, this is still a bit questionable, so we will try another combination of models."
      ],
      "metadata": {
        "id": "zTvJMKR1Yfnz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c1f8ef6"
      },
      "source": [
        "X_meta_val = np.concatenate((y_val_pred_proba.reshape(-1, 1), y_val_pred_proba_best_xgb.reshape(-1, 1)), axis=1)\n",
        "\n",
        "print(\"Shape of X_meta_val:\", X_meta_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb93ed18"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Instantiate a Logistic Regression meta-model\n",
        "meta_model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Train the meta-model using the combined validation predictions and actual labels\n",
        "meta_model.fit(X_meta_val, y_val)\n",
        "\n",
        "print(\"Meta-model (Logistic Regression) training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict probabilities on the validation meta-features using the trained meta-model\n",
        "meta_model_pred_proba = meta_model.predict_proba(X_meta_val)[:, 1]\n",
        "\n",
        "# Calculate the AUC ROC score for the hybrid model\n",
        "hybrid_stacked_auc = roc_auc_score(y_val, meta_model_pred_proba)\n",
        "\n",
        "print(f\"AUC ROC for Hybrid Stacked Model (Logistic Regression + XGBoost): {hybrid_stacked_auc}\")\n",
        "\n",
        "# Calculate the ROC curve for the hybrid stacked model\n",
        "fpr_hybrid_stacked, tpr_hybrid_stacked, thresholds_hybrid_stacked = roc_curve(y_val, meta_model_pred_proba)\n",
        "\n",
        "# Plot the ROC curve for the hybrid stacked model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_hybrid_stacked, tpr_hybrid_stacked, color='darkorange', lw=2, label=f'ROC curve (AUC = {hybrid_stacked_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Hybrid Stacked Model')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XQ2zQQoZ2Lmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ba3ed60"
      },
      "source": [
        "logistic_train_pred_proba = logistic_model.predict_proba(X_processed)[:, 1]\n",
        "xgb_train_pred_proba = best_xgb_model.predict_proba(X_processed_df)[:, 1]\n",
        "\n",
        "print(\"Logistic Regression training predictions generated.\")\n",
        "print(\"XGBoost training predictions generated.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de270b63"
      },
      "source": [
        "X_meta_train = np.concatenate((logistic_train_pred_proba.reshape(-1, 1), xgb_train_pred_proba.reshape(-1, 1)), axis=1)\n",
        "\n",
        "print(\"Shape of X_meta_train:\", X_meta_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59fc3276"
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict probabilities for the positive class on X_meta_train using the trained meta-model\n",
        "hybrid_stacked_train_pred_proba = meta_model.predict_proba(X_meta_train)[:, 1]\n",
        "\n",
        "# Calculate the AUC ROC score for the hybrid stacked model on the training set\n",
        "hybrid_stacked_train_auc = roc_auc_score(y, hybrid_stacked_train_pred_proba)\n",
        "\n",
        "print(f\"AUC ROC for Hybrid Stacked Model on Training Set: {hybrid_stacked_train_auc}\")\n",
        "\n",
        "# Calculate the ROC curve for the hybrid stacked model on the training set\n",
        "fpr_hybrid_stacked_train, tpr_hybrid_stacked_train, thresholds_hybrid_stacked_train = roc_curve(y, hybrid_stacked_train_pred_proba)\n",
        "\n",
        "# Plot the ROC curve for the hybrid stacked model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_hybrid_stacked_train, tpr_hybrid_stacked_train, color='darkorange', lw=2, label=f'ROC curve (AUC = {hybrid_stacked_train_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Hybrid Stacked Model on Training Set')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LR-XGBoost model shows the highest AUC ROC based on the training set alone yet, with a value of ~0.9229. However, based on the validation set, the first hybrid model performed exceedingly high. Let's try one more hybrid model to see if we can strike a balance."
      ],
      "metadata": {
        "id": "DqZa_dw23Ubc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50524340"
      },
      "source": [
        "import cupy as cp\n",
        "\n",
        "# 1. Generate probability predictions from the best_xgb_model on X_val_df\n",
        "y_val_pred_proba_xgb_hybrid = best_xgb_model.predict_proba(X_val_df)[:, 1]\n",
        "print(\"XGBoost predictions generated on validation set.\")\n",
        "\n",
        "# 2. Generate probability predictions from the final_nb_model on X_val_nb_gpu\n",
        "y_val_pred_proba_nb_hybrid_gpu = final_nb_model.predict_proba(X_val_nb_gpu)[:, 1]\n",
        "print(\"cuML Gaussian Naive Bayes (GPU) predictions generated on validation set.\")\n",
        "\n",
        "# 3. Convert y_val_pred_proba_nb_hybrid_gpu (CuPy array) to a NumPy array\n",
        "y_val_pred_proba_nb_hybrid_cpu = cp.asnumpy(y_val_pred_proba_nb_hybrid_gpu)\n",
        "print(\"cuML Gaussian Naive Bayes predictions converted to CPU NumPy array.\")\n",
        "\n",
        "print(f\"Shape of y_val_pred_proba_xgb_hybrid: {y_val_pred_proba_xgb_hybrid.shape}\")\n",
        "print(f\"Shape of y_val_pred_proba_nb_hybrid_cpu: {y_val_pred_proba_nb_hybrid_cpu.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c265d75"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Combine the predictions to form meta-features for the meta-model\n",
        "X_meta_val_hybrid = np.column_stack((y_val_pred_proba_xgb_hybrid, y_val_pred_proba_nb_hybrid_cpu))\n",
        "\n",
        "print(f\"Shape of meta-features for validation set: {X_meta_val_hybrid.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c84ca5ad"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Instantiate a Logistic Regression meta-model\n",
        "meta_model_hybrid = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Train the meta-model using the combined validation predictions and actual labels\n",
        "meta_model_hybrid.fit(X_meta_val_hybrid, y_val) # y_val is the true labels for the validation set\n",
        "\n",
        "print(\"Meta-model (Logistic Regression) training complete using XGBoost and Naive Bayes predictions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcc7a07a"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict probabilities using the trained meta-model\n",
        "y_val_pred_proba_hybrid_stacked = meta_model_hybrid.predict_proba(X_meta_val_hybrid)[:, 1]\n",
        "\n",
        "# Calculate and print the AUC ROC score\n",
        "hybrid_stacked_auc_xgb_nb = roc_auc_score(y_val, y_val_pred_proba_hybrid_stacked)\n",
        "print(f\"\\nAUC ROC for Hybrid Stacked Model (XGBoost + Naive Bayes) on Validation Set: {hybrid_stacked_auc_xgb_nb}\")\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr_hybrid_stacked_xgb_nb, tpr_hybrid_stacked_xgb_nb, _ = roc_curve(y_val, y_val_pred_proba_hybrid_stacked)\n",
        "roc_auc_hybrid_stacked_xgb_nb = auc(fpr_hybrid_stacked_xgb_nb, tpr_hybrid_stacked_xgb_nb)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_hybrid_stacked_xgb_nb, tpr_hybrid_stacked_xgb_nb, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_hybrid_stacked_xgb_nb:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Hybrid Stacked Model (XGBoost + Naive Bayes)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Artificial Dataset Testing and Final Model Selection"
      ],
      "metadata": {
        "id": "UAujOzTa8h6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the single AUC ROC scores alone, it is difficult to choose a model that will generalize to unseen data. Each XGB hybrid performs rather well, but we can use artificial data to find which model scores best on average.\n",
        "\n",
        "First we'll create the synthetic data:"
      ],
      "metadata": {
        "id": "aqpVjLpk8wJz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "534a2e33"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"Imported make_classification and train_test_split.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c953b540"
      },
      "source": [
        "synthetic_dataset_configs = [\n",
        "    {\n",
        "        'n_samples': 1000,\n",
        "        'n_features': 20,\n",
        "        'n_informative': 10,\n",
        "        'n_redundant': 5,\n",
        "        'n_clusters_per_class': 2,\n",
        "        'random_state': 42\n",
        "    },\n",
        "    {\n",
        "        'n_samples': 2000,\n",
        "        'n_features': 50,\n",
        "        'n_informative': 15,\n",
        "        'n_redundant': 10,\n",
        "        'n_clusters_per_class': 3,\n",
        "        'random_state': 123\n",
        "    },\n",
        "    {\n",
        "        'n_samples': 500,\n",
        "        'n_features': 10,\n",
        "        'n_informative': 5,\n",
        "        'n_redundant': 2,\n",
        "        'n_clusters_per_class': 1,\n",
        "        'random_state': 789\n",
        "    },\n",
        "    {\n",
        "        'n_samples': 1500,\n",
        "        'n_features': 30,\n",
        "        'n_informative': 20,\n",
        "        'n_redundant': 5,\n",
        "        'n_clusters_per_class': 2,\n",
        "        'random_state': 987\n",
        "    }\n",
        "]\n",
        "\n",
        "synthetic_datasets = []\n",
        "\n",
        "for i, config in enumerate(synthetic_dataset_configs):\n",
        "    print(f\"Generating dataset {i+1} with config: {config}\")\n",
        "    X_synthetic, y_synthetic = make_classification(**config)\n",
        "\n",
        "    X_train_synthetic, X_test_synthetic, y_train_synthetic, y_test_synthetic = train_test_split(\n",
        "        X_synthetic, y_synthetic, test_size=0.2, random_state=42, stratify=y_synthetic\n",
        "    )\n",
        "\n",
        "    synthetic_datasets.append({\n",
        "        'config': config,\n",
        "        'X_train': X_train_synthetic,\n",
        "        'X_test': X_test_synthetic,\n",
        "        'y_train': y_train_synthetic,\n",
        "        'y_test': y_test_synthetic\n",
        "    })\n",
        "\n",
        "    print(f\"  Dataset {i+1} shapes: \")\n",
        "    print(f\"    X_train: {X_train_synthetic.shape}, y_train: {y_train_synthetic.shape}\")\n",
        "    print(f\"    X_test: {X_test_synthetic.shape}, y_test: {y_test_synthetic.shape}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(f\"Successfully created {len(synthetic_datasets)} synthetic datasets.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d0edd0a"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "for i, dataset in enumerate(synthetic_datasets):\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit and transform X_train\n",
        "    dataset['X_train'] = scaler.fit_transform(dataset['X_train'])\n",
        "\n",
        "    # Transform X_test\n",
        "    dataset['X_test'] = scaler.transform(dataset['X_test'])\n",
        "\n",
        "    print(f\"Dataset {i+1} scaled. New shapes: \")\n",
        "    print(f\"  X_train: {dataset['X_train'].shape}, X_test: {dataset['X_test'].shape}\")\n",
        "\n",
        "print(\"Preprocessing with StandardScaler complete for all synthetic datasets.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be287544"
      },
      "source": [
        "Now, we'll train the models on this artificial data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70fb51e9"
      },
      "source": [
        "import xgboost as xgb\n",
        "from cuml.ensemble import RandomForestClassifier as cumlRandomForestClassifier\n",
        "from cuml.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "retrained_hybrid_results = []\n",
        "print(\"Initialized retrained_hybrid_results list.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92dbaa62"
      },
      "source": [
        "for i, dataset in enumerate(synthetic_datasets):\n",
        "    print(f\"\\nProcessing synthetic dataset {i+1}...\")\n",
        "    X_train_synth, X_test_synth, y_train_synth, y_test_synth = (\n",
        "        dataset['X_train'], dataset['X_test'], dataset['y_train'], dataset['y_test']\n",
        "    )\n",
        "\n",
        "    # b. Retrain Base XGBoost Model\n",
        "    xgb_tuned_params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'random_state': 42,\n",
        "        'eval_metric': 'logloss',\n",
        "        'tree_method': 'hist',\n",
        "        'device': 'cuda',\n",
        "        'n_estimators': grid_search_xgb.best_params_['n_estimators'],\n",
        "        'learning_rate': grid_search_xgb.best_params_['learning_rate'],\n",
        "        'max_depth': grid_search_xgb.best_params_['max_depth'],\n",
        "        'subsample': grid_search_xgb.best_params_['subsample'],\n",
        "        'colsample_bytree': grid_search_xgb.best_params_['colsample_bytree']\n",
        "    }\n",
        "    xgb_model_synth = xgb.XGBClassifier(**xgb_tuned_params)\n",
        "    xgb_model_synth.fit(X_train_synth, y_train_synth)\n",
        "    xgb_train_preds = xgb_model_synth.predict_proba(X_train_synth)[:, 1]\n",
        "    xgb_test_preds = xgb_model_synth.predict_proba(X_test_synth)[:, 1]\n",
        "    print(f\"  XGBoost trained. AUC on test: {roc_auc_score(y_test_synth, xgb_test_preds):.4f}\")\n",
        "\n",
        "    # c. Retrain Base cuml Random Forest Model\n",
        "    rf_tuned_params = {\n",
        "        'n_estimators': grid_search_cuml.best_params_['n_estimators'],\n",
        "        'max_depth': grid_search_cuml.best_params_['max_depth'],\n",
        "        'min_samples_split': grid_search_cuml.best_params_['min_samples_split'],\n",
        "        'random_state': 42\n",
        "    }\n",
        "    rf_model_synth = cumlRandomForestClassifier(**rf_tuned_params)\n",
        "\n",
        "    X_train_rf_gpu = cp.asarray(X_train_synth.astype(np.float32))\n",
        "    y_train_rf_gpu = cp.asarray(y_train_synth.astype(np.float32))\n",
        "    X_test_rf_gpu = cp.asarray(X_test_synth.astype(np.float32))\n",
        "\n",
        "    rf_model_synth.fit(X_train_rf_gpu, y_train_rf_gpu)\n",
        "    rf_train_preds_gpu = rf_model_synth.predict_proba(X_train_rf_gpu)[:, 1]\n",
        "    rf_train_preds_cpu = cp.asnumpy(rf_train_preds_gpu)\n",
        "    rf_test_preds_gpu = rf_model_synth.predict_proba(X_test_rf_gpu)[:, 1]\n",
        "    rf_test_preds_cpu = cp.asnumpy(rf_test_preds_gpu)\n",
        "    print(f\"  cuML Random Forest trained. AUC on test: {roc_auc_score(y_test_synth, rf_test_preds_cpu):.4f}\")\n",
        "\n",
        "    # d. Retrain Base cuml Gaussian Naive Bayes Model\n",
        "    nb_model_synth = GaussianNB(var_smoothing=best_params_nb['var_smoothing'])\n",
        "\n",
        "    X_train_nb_gpu = cp.asarray(X_train_synth.astype(np.float32))\n",
        "    y_train_nb_gpu = cp.asarray(y_train_synth.astype(np.float32))\n",
        "    X_test_nb_gpu = cp.asarray(X_test_synth.astype(np.float32))\n",
        "\n",
        "    nb_model_synth.fit(X_train_nb_gpu, y_train_nb_gpu)\n",
        "    nb_train_preds_gpu = nb_model_synth.predict_proba(X_train_nb_gpu)[:, 1]\n",
        "    nb_train_preds_cpu = cp.asnumpy(nb_train_preds_gpu)\n",
        "    nb_test_preds_gpu = nb_model_synth.predict_proba(X_test_nb_gpu)[:, 1]\n",
        "    nb_test_preds_cpu = cp.asnumpy(nb_test_preds_gpu)\n",
        "    print(f\"  cuML Naive Bayes trained. AUC on test: {roc_auc_score(y_test_synth, nb_test_preds_cpu):.4f}\")\n",
        "\n",
        "    # e. Retrain Base Logistic Regression Model\n",
        "    lr_model_synth = LogisticRegression(solver='liblinear', random_state=42)\n",
        "    lr_model_synth.fit(X_train_synth, y_train_synth)\n",
        "    lr_train_preds = lr_model_synth.predict_proba(X_train_synth)[:, 1]\n",
        "    lr_test_preds = lr_model_synth.predict_proba(X_test_synth)[:, 1]\n",
        "    print(f\"  Logistic Regression trained. AUC on test: {roc_auc_score(y_test_synth, lr_test_preds):.4f}\")\n",
        "\n",
        "    # f. Hybrid Model 1: XGBoost + Random Forest (Simple Averaging)\n",
        "    hybrid1_test_preds = (xgb_test_preds + rf_test_preds_cpu) / 2\n",
        "    hybrid1_auc = roc_auc_score(y_test_synth, hybrid1_test_preds)\n",
        "    print(f\"  Hybrid 1 (XGB+RF Avg) AUC on test: {hybrid1_auc:.4f}\")\n",
        "\n",
        "    # g. Hybrid Model 2: Logistic Regression + XGBoost (Stacked)\n",
        "    X_meta_train_h2 = np.column_stack((lr_train_preds, xgb_train_preds))\n",
        "    meta_model_h2 = LogisticRegression(solver='liblinear', random_state=42)\n",
        "    meta_model_h2.fit(X_meta_train_h2, y_train_synth)\n",
        "    X_meta_test_h2 = np.column_stack((lr_test_preds, xgb_test_preds))\n",
        "    hybrid2_test_preds = meta_model_h2.predict_proba(X_meta_test_h2)[:, 1]\n",
        "    hybrid2_auc = roc_auc_score(y_test_synth, hybrid2_test_preds)\n",
        "    print(f\"  Hybrid 2 (LR+XGB Stacked) AUC on test: {hybrid2_auc:.4f}\")\n",
        "\n",
        "    # h. Hybrid Model 3: XGBoost + Naive Bayes (Stacked)\n",
        "    X_meta_train_h3 = np.column_stack((xgb_train_preds, nb_train_preds_cpu))\n",
        "    meta_model_h3 = LogisticRegression(solver='liblinear', random_state=42)\n",
        "    meta_model_h3.fit(X_meta_train_h3, y_train_synth)\n",
        "    X_meta_test_h3 = np.column_stack((xgb_test_preds, nb_test_preds_cpu))\n",
        "    hybrid3_test_preds = meta_model_h3.predict_proba(X_meta_test_h3)[:, 1]\n",
        "    hybrid3_auc = roc_auc_score(y_test_synth, hybrid3_test_preds)\n",
        "    print(f\"  Hybrid 3 (XGB+NB Stacked) AUC on test: {hybrid3_auc:.4f}\")\n",
        "\n",
        "    retrained_hybrid_results.append({\n",
        "        'config': dataset['config'],\n",
        "        'XGB_RF_Avg_Test_Preds': hybrid1_test_preds,\n",
        "        'LR_XGB_Stacked_Test_Preds': hybrid2_test_preds,\n",
        "        'XGB_NB_Stacked_Test_Preds': hybrid3_test_preds,\n",
        "        'XGB_RF_Avg_AUC': hybrid1_auc,\n",
        "        'LR_XGB_Stacked_AUC': hybrid2_auc,\n",
        "        'XGB_NB_Stacked_AUC': hybrid3_auc\n",
        "    })\n",
        "\n",
        "print(\"\\nFinished retraining and evaluating hybrid models on all synthetic datasets.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6e9f1c0"
      },
      "source": [
        "print(\"\\n--- Hybrid Model Evaluation on Artificial Datasets ---\")\n",
        "for i, result in enumerate(retrained_hybrid_results):\n",
        "    print(f\"\\nDataset {i+1} Configuration:\")\n",
        "    for key, value in result['config'].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(f\"  AUC ROC - XGBoost + Random Forest (Simple Averaging): {result['XGB_RF_Avg_AUC']:.4f}\")\n",
        "    print(f\"  AUC ROC - Logistic Regression + XGBoost (Stacked): {result['LR_XGB_Stacked_AUC']:.4f}\")\n",
        "    print(f\"  AUC ROC - XGBoost + Naive Bayes (Stacked): {result['XGB_NB_Stacked_AUC']:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally the summary of results:"
      ],
      "metadata": {
        "id": "4eiZhlo09hb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Calculate Average AUC ROC for Hybrid Models on Artificial Datasets ---\n",
        "hybrid1_aucs = [result['XGB_RF_Avg_AUC'] for result in retrained_hybrid_results]\n",
        "hybrid2_aucs = [result['LR_XGB_Stacked_AUC'] for result in retrained_hybrid_results]\n",
        "hybrid3_aucs = [result['XGB_NB_Stacked_AUC'] for result in retrained_hybrid_results]\n",
        "\n",
        "mean_hybrid1_auc = np.mean(hybrid1_aucs)\n",
        "mean_hybrid2_auc = np.mean(hybrid2_aucs)\n",
        "mean_hybrid3_auc = np.mean(hybrid3_aucs)\n",
        "\n",
        "# --- 2. Gather Individual Model AUC ROC scores on Original Validation Set ---\n",
        "# From previous cells, these variables should be available\n",
        "# Logistic Regression (from 5-fold CV on original training data for robustness) - using mean_auc\n",
        "# Random Forest (tuned with cuml and GridSearchCV) - using grid_search_cuml.best_score_\n",
        "# Neural Network (tuned) - using best_auc_nn\n",
        "# XGBoost (tuned with GridSearchCV) - using best_xgb_auc\n",
        "# LightGBM (single model) - using lgbm_auc\n",
        "# TabNet (single model) - manually entered 0.9119 from previous output\n",
        "# Gaussian Naive Bayes (tuned with cuml) - using final_nb_auc\n",
        "\n",
        "\n",
        "# Create a DataFrame for comparison\n",
        "comparison_data = {\n",
        "    'Model': [\n",
        "        'Hybrid: XGB+RF (Avg) - Artificial Data Avg',\n",
        "        'Hybrid: LR+XGB (Stacked) - Artificial Data Avg',\n",
        "        'Hybrid: XGB+NB (Stacked) - Artificial Data Avg',\n",
        "        'XGBoost (Tuned) - Original Val Set',\n",
        "        'Random Forest (Tuned) - Original Val Set',\n",
        "        'Logistic Regression (CV) - Original Val Set',\n",
        "        'Neural Network (Tuned) - Original Val Set',\n",
        "        'LightGBM - Original Val Set',\n",
        "        'TabNet - Original Val Set',\n",
        "        'Gaussian Naive Bayes (Tuned) - Original Val Set'\n",
        "    ],\n",
        "    'AUC_ROC': [\n",
        "        mean_hybrid1_auc,\n",
        "        mean_hybrid2_auc,\n",
        "        mean_hybrid3_auc,\n",
        "        best_xgb_auc,\n",
        "        grid_search_cuml.best_score_,\n",
        "        mean_auc, # Using mean_auc from LR CV\n",
        "        best_auc_nn,\n",
        "        lgbm_auc,\n",
        "        0.9119, # TabNet AUC from text output\n",
        "        final_nb_auc\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df_sorted = comparison_df.sort_values(by='AUC_ROC', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# --- 3. Print the average scores and individual scores ---\n",
        "print(\"\\n--- Average AUC ROC Scores for Hybrid Models on Artificial Datasets ---\")\n",
        "print(f\"XGBoost + Random Forest (Simple Averaging): {mean_hybrid1_auc:.4f}\")\n",
        "print(f\"Logistic Regression + XGBoost (Stacked):     {mean_hybrid2_auc:.4f}\")\n",
        "print(f\"XGBoost + Naive Bayes (Stacked):             {mean_hybrid3_auc:.4f}\")\n",
        "\n",
        "print(\"\\n--- Individual Model AUC ROC Scores on Original Validation Set ---\")\n",
        "print(f\"XGBoost (Tuned):                       {best_xgb_auc:.4f}\")\n",
        "print(f\"Random Forest (Tuned):                 {grid_search_cuml.best_score_:.4f}\")\n",
        "print(f\"Logistic Regression (CV):              {mean_auc:.4f}\")\n",
        "print(f\"Neural Network (Tuned):                {best_auc_nn:.4f}\")\n",
        "print(f\"LightGBM:                              {lgbm_auc:.4f}\")\n",
        "print(f\"TabNet:                                {0.9119:.4f}\")\n",
        "print(f\"Gaussian Naive Bayes (Tuned):          {final_nb_auc:.4f}\")\n",
        "\n",
        "print(\"\\n--- All Models Ranked by AUC ROC ---\")\n",
        "print(comparison_df_sorted.to_string(index=False))\n",
        "\n",
        "# --- 4. Create a Bar Chart for Comparison ---\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.barplot(x='AUC_ROC', y='Model', data=comparison_df_sorted, palette='viridis')\n",
        "plt.title('Comparison of Hybrid and Individual Model AUC ROC Scores')\n",
        "plt.xlabel('AUC ROC Score')\n",
        "plt.ylabel('Model Type')\n",
        "plt.xlim(0.85, 1.0) # Adjust x-axis to better show differences\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YYZLueLz9Ii_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "\n",
        "# 1. Generate Training Predictions for XGB+NB Stacked Model\n",
        "# Base predictions from best_xgb_model on X_processed_df\n",
        "xgb_train_preds_for_nb_stack = best_xgb_model.predict_proba(X_processed_df)[:, 1]\n",
        "\n",
        "# Base predictions from final_nb_model on X_processed_gpu\n",
        "# The original X_processed (from preprocessor.fit_transform(X)) has 60 features.\n",
        "# X_processed_df has 64 features due to feature engineering for XGBoost.\n",
        "# The final_nb_model was trained on 60 features.\n",
        "# Therefore, we need to convert X_processed to CuPy, not X_processed_df.\n",
        "\n",
        "# Ensure X_processed is dense for CuPy conversion if it's sparse\n",
        "X_processed_dense_np = X_processed.toarray().astype(np.float32) if hasattr(X_processed, 'toarray') else X_processed.astype(np.float32)\n",
        "X_processed_nb_gpu = cp.asarray(X_processed_dense_np)\n",
        "\n",
        "nb_train_preds_for_nb_stack_gpu = final_nb_model.predict_proba(X_processed_nb_gpu)[:, 1]\n",
        "nb_train_preds_for_nb_stack_cpu = cp.asnumpy(nb_train_preds_for_nb_stack_gpu)\n",
        "\n",
        "# Combine these predictions to form meta-features for the meta-model\n",
        "X_meta_train_xgb_nb = np.column_stack((xgb_train_preds_for_nb_stack, nb_train_preds_for_nb_stack_cpu))\n",
        "\n",
        "# Use meta_model_hybrid (meta-learner for XGB+NB) to predict on these meta-features\n",
        "xgb_nb_stacked_train_pred_proba = meta_model_hybrid.predict_proba(X_meta_train_xgb_nb)[:, 1]\n",
        "\n",
        "# 2. Calculate AUC ROC for XGB+NB Stacked Model on Training Set\n",
        "xgb_nb_stacked_train_auc = roc_auc_score(y, xgb_nb_stacked_train_pred_proba)\n",
        "print(f\"AUC ROC for XGB+NB Stacked Model on Training Set: {xgb_nb_stacked_train_auc:.4f}\")\n",
        "\n",
        "# 3. Gather all three hybrid models' training AUC ROC scores\n",
        "# hybrid_stacked_train_auc (for LR+XGB Stacked) - from cell 59fc3276\n",
        "# hybrid_train_auc (for XGB+RF Avg) - from cell LGqcdsiLW_5z\n",
        "# xgb_nb_stacked_train_auc (for XGB+NB Stacked) - calculated above\n",
        "\n",
        "comparison_data_train = {\n",
        "    'Hybrid Model': [\n",
        "        'LR+XGB (Stacked) - Training Set',\n",
        "        'XGB+RF (Avg) - Training Set',\n",
        "        'XGB+NB (Stacked) - Training Set'\n",
        "    ],\n",
        "    'AUC_ROC (Training Set)': [\n",
        "        hybrid_stacked_train_auc,\n",
        "        hybrid_train_auc,\n",
        "        xgb_nb_stacked_train_auc\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df_train = pd.DataFrame(comparison_data_train)\n",
        "comparison_df_train_sorted = comparison_df_train.sort_values(by='AUC_ROC (Training Set)', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n--- Hybrid Models Ranked by AUC ROC on Training Data ---\")\n",
        "print(comparison_df_train_sorted.to_string(index=False))\n",
        "\n",
        "# Create a Bar Chart for Comparison\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='AUC_ROC (Training Set)', y='Hybrid Model', data=comparison_df_train_sorted, palette='viridis')\n",
        "plt.title('AUC ROC of Hybrid Models on Training Data')\n",
        "plt.xlabel('AUC ROC Score')\n",
        "plt.ylabel('Hybrid Model Type')\n",
        "plt.xlim(0.8, 1.0) # Adjust x-axis to better show differences\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9WNuXCURHOSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest score for average performance on artificial data is just barely won by the logistic regression XGBoost model. With the XGB/RF and XGB/NB there were also some less consistent performances, particularly between the XGB/RF's validation and training scores, which would cause a bit more of a risk in terms of the performance on actual test data.\n",
        "\n",
        "So, the final model will be the **Stacked Logistic Regression + XGBoost model.** Now we can create a submission."
      ],
      "metadata": {
        "id": "efyTx2eR9yz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.DataFrame({'id': test_df['id'], 'loan_paid_back': stacked_lr_xgb_test_preds})\n",
        "\n",
        "# Save the submission_df to a CSV file named 'submission.csv', excluding the index\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file 'submission.csv' created successfully using the LR/XGB stacked model.\")\n",
        "print(submission_df.head())"
      ],
      "metadata": {
        "id": "BSg_J-gd-nhx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}